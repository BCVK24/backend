<h2>VK Audio Clean (ML)</h2>

Данный репозиторий предоставляет интерфейс для взаимодействия с ASR-моделями GigaAM (Сбер) и Whisper (OpenAI). Интерфейс для взаимодействия из сторонних модулей находится в классе `ASRInference` в файле `asr_inference.py`. Реализация приватных функций для работы с моделями находится в классе `ModelAbstract` в файле `model_abstract.py`

Класс `ASRInference` состоит из 3-х публичных методов: `forced_align`, `transcribe`, `filter`: <br><br>Метод `transcribe` нужен для транскрибации wav-аудиозиписи, на вход он получает название файла, а на выходе возвращает список токенов всей аудиозаписи либо ее сегментов. <br><br>Метод `forced_align` используется для получения временного распределения токенов на аудиозаписи, на вход он получает название файла, на выходе метод выдаёт список списков, где каждый подсписок состоит из токена, времени начала произношения этого токена на аудиозаписи, время конца.<br><br> Метод `filter` используется для фильтрации списка выравненных по времени токенов (выхода функции `forced_align`), выводит список списков, где каждый подсписок является списком из 4-х элементов: токена, временной метки начала, временной метки конца, bool-значения, говорящего о том, является ли токен мусорным.

<br>

При использования класса ASRInference необходимо передать название модели (str), которую вы хотите использовать (`GigaAM`, `Whisper` и тд.), а также конфиг (dict). Также перед передачей wav-файла в ASRInference, необходимо предобработать его с помощью метода `convert_wav` из `utils.py`, который преобразует wav-файл из одного канала в другой, а также меняет герцовку.

```
from model_abstract import ModelAbstract
from utils import convert_wav
from nemo.collections.asr.modules.audio_preprocessing import AudioToMelSpectrogramPreprocessor

model_config = {'model_config': 'conf/ctc_model_config.yaml',
            'model_weights': '../ml/ctc_model_weights.ckpt'}
SPEECH_FILE = "data_for_tests/test_60.wav"
speech = convert_wav(SPEECH_FILE)

asr = ModelAbstract('GigaAM', model_config)
alignments = asr.forced_align(speech)
alignments_filter = asr.filter(alignments)

print(alignments_filter)
```

Этот пример выдаёт следующий список списков выравнивания:

```[['то', 0.0, 0.2398125, False], ['как', 0.31975, 0.4396875, False], ['мы', 0.4796875, 0.559625, False], ['его', 0.6395625, 0.7595, False], ['воспринимаем', 0.8794375, 1.559, False], ['нравится', 1.6789375, 2.118625, False], ['он', 2.2385625, 2.3585, False], ['нам', 2.4384375, 2.558375, False], ['или', 2.6783125, 2.79825, False], ['нет', 2.918125, 3.0780625, False], ['кажется', 3.198, 3.51775, False], ['он', 3.59775, 3.717625, False], ['песклявым', 3.9175, 4.597125, False], ['или', 4.6770625, 4.8369375, False], ['наоборот', 4.8769375, 5.2766875, False], ['бархатным', 5.396625, 5.8363125, False], ['глубоким', 5.95625, 6.356, False], ['и', 6.4359375, 6.4759375, False], ['красивым', 6.555875, 7.0355625, False], ['это', 7.1955, 7.3154375, False], ['наверное', 7.395375, 7.795125, False]]```